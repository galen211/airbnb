{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup paths\n",
    "- csv data in the `data` path\n",
    "- saved models in the `data/model` path\n",
    "- python scripts in the `lib` path\n",
    "- notebooks demonstrating model in `doc` path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root path: /Users/galen/Documents/airbnb\n",
      "data path: /Users/galen/Documents/airbnb/data\n",
      "model path: /Users/galen/Documents/airbnb/data/model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "\n",
    "nb_path = os.getcwd() # should be the location of the pynb file\n",
    "root_path = os.path.split(nb_path)[0] # project root\n",
    "data_path = os.path.join(root_path, \"data\") # for data\n",
    "lib_path = os.path.join(root_path, \"lib\") # for python scripts\n",
    "model_path = os.path.join(root_path, \"data\", \"model\")\n",
    "\n",
    "print(\"root path: \" + root_path)\n",
    "print(\"data path: \" + data_path)\n",
    "print(\"model path: \" + model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries\n",
    "The libraries we will use for our analysis are:\n",
    "- `Numpy`\n",
    "- `Pandas`\n",
    "- `nltk`\n",
    "- `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import pickle\n",
    "import datetime\n",
    "from geopy.distance import vincenty\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read `train` and `test` csv files\n",
    "We concatenate the training and test files into a master data frame called `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(data_path, \"train.csv\"))\n",
    "test = pd.read_csv(os.path.join(data_path, \"test.csv\"))\n",
    "\n",
    "train['dataset'] = 'train'\n",
    "test['dataset'] = 'test'\n",
    "\n",
    "data = pd.concat([train,test], axis = 0).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Processing on Description\n",
    "We apply word stemming to normalize the text, which allows us to group words that are related like \"beautiful\" and \"beautifully\".  In English, the stems of words are usually consistent in the sentiment they convey.  After normalizing the words in the descriptions, we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource 'corpora/words' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/galen/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/Users/galen/anaconda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/galen/anaconda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/words.zip/words/' not found.  Please use the\n  NLTK Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/galen/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3a3912a48fda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mwpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordPunctTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0menglish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/galen/anaconda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/galen/anaconda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/galen/anaconda/lib/python3.6/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/galen/anaconda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/words' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/galen/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "desc = data[['id', 'description']]\n",
    "\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "english = set(nltk.corpus.words.words())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return ''\n",
    "    else:\n",
    "        # re.sub(r'[^a-zA-Z\\s]', '', a, re.I|re.A) --> delete all punctuation and\n",
    "        # non-character letters\n",
    "        # strip = strip of trailing and leading spaces\n",
    "        # lower = make all lowercase\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A).lower().strip()\n",
    "        # tokenize\n",
    "        tokens = wpt.tokenize(text)\n",
    "        # filter stopwords out of document\n",
    "        filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens \n",
    "                           if ((token not in stop_words) & (token in english))]\n",
    "        # re-create document from filtered tokens\n",
    "        text = ' '.join(filtered_tokens)\n",
    "        return text\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_text)\n",
    "\n",
    "desc['normtext'] = [normalize_corpus(x) for x in desc['description']]\n",
    "\n",
    "# Change normtext to a list of strings\n",
    "tlist = []\n",
    "for i in desc['normtext']:\n",
    "    tlist.append(str(i))\n",
    "    \n",
    "allwords = ' '.join(tlist)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(min_df=0., max_df=1., max_features=10000)\n",
    "cv_matrix1 = cv.fit_transform(tlist)\n",
    "cv_matrix1 = cv_matrix1.toarray()\n",
    "headers = np.array(cv.get_feature_names())\n",
    "\n",
    "# concat headers and matrix\n",
    "words = pd.DataFrame(cv_matrix1, columns=headers)\n",
    "\n",
    "# get the words that matter\n",
    "view = ['view', 'panoramic']\n",
    "walk = ['walk', 'walkable', 'walking']\n",
    "positive = ['great', 'good']\n",
    "pos_sent = ['inviting', 'colorful', 'gold', 'sensitive', 'desirable', 'plush', 'romantic', 'adorable', 'tastefully','tasteful',\n",
    "            'cute', 'nicely', 'pleasure', 'favorite', 'quintessential', 'pretty', 'interesting', 'lovely', 'enjoy', 'enjoying']\n",
    "very_pos = ['best', 'perfect', 'perfectly', 'amazing', 'super', 'wonderful', 'grand', 'awesome', 'excellent', \n",
    "            'fantastic', 'incredible', 'spectacular', 'fabulous', 'gem', 'authentic', 'rare', 'exciting', \n",
    "            'incredibly', 'rich', 'dream', 'magnificent', 'deluxe', 'scenic', 'picturesque', 'exceptional', 'paradise']\n",
    "new = ['new', 'newly']\n",
    "big = ['big', 'large', 'huge', 'giant', 'massive', 'sweeping', 'enormous', 'expansive', 'roomy']\n",
    "near = ['near', 'close', 'nearby', 'proximity', 'adjacent']\n",
    "minutes = ['minute', 'min']\n",
    "quiet = ['quiet', 'peaceful', 'serene', 'tranquil', 'calm', 'peace']\n",
    "beautiful = ['beautiful', 'beautifully', 'gorgeous', 'beauty']\n",
    "city = ['city', 'urban', 'metropolitan']\n",
    "comfortable = ['comfortable', 'comfy', 'comfortably', 'comfort']\n",
    "cozy = ['cozy', 'coziness', 'homey', 'cosy']\n",
    "central = ['central', 'center', 'centrally']\n",
    "many = ['many', 'numerous', 'ample', 'plenty']\n",
    "shopping = ['shopping', 'shop', 'store', 'plaza', 'bakery']\n",
    "porch = ['porch', 'patio', 'deck']\n",
    "entire = ['entire', 'whole']\n",
    "couch = ['couch', 'sofa']\n",
    "historic = ['historic', 'history', 'historical']\n",
    "outdoors = ['outdoors', 'mountain', 'outdoor', 'lush', 'trail', 'nature', 'glen', 'forest', 'valley', 'apple', 'oak',\n",
    "            'sanctuary', 'bike', 'sunset', 'natural']\n",
    "sunny = ['sunny', 'bright', 'sun', 'sunlight', 'sunshine', 'sunlit']\n",
    "easy = ['easy', 'convenient', 'convenience', 'conveniently']\n",
    "luxury = ['luxury', 'luxurious', 'penthouse', 'oasis', 'gated', 'resort', 'villa', 'estate', 'mansion']\n",
    "charming = ['charming', 'charm']\n",
    "near_ocean = ['ocean', 'pacific', 'bay', 'marina', 'pier', 'waterfront', 'sand', 'harbor', 'wharf', 'boat', 'shore', \n",
    "              'seaport', 'alcove', 'coast', 'surf']\n",
    "near_small_water = ['river', 'lake', 'ferry', 'riverside', 'pond', 'creek', 'canal']\n",
    "famous = ['famous', 'fame', 'iconic']\n",
    "relax = ['relax','relaxed', 'relaxation', 'unwind']\n",
    "fast = ['fast', 'quick', 'quickly']\n",
    "modern = ['modern', 'contemporary']\n",
    "vacation = ['vacation', 'retreat']\n",
    "busy = ['busy', 'bustling', 'bustle', 'lively', 'hustle']\n",
    "stone = ['stone', 'marble', 'granite']\n",
    "upscale = ['upscale', 'elegant', 'chic', 'exclusive', 'premium', 'prestigious', 'fancy', 'expensive', 'vanity', 'fashion']\n",
    "architecture = ['architecture', 'architectural']\n",
    "vintage = ['vintage', 'antique']\n",
    "culture = ['culture', 'cultural']\n",
    "fresh = ['fresh', 'freshly']\n",
    "sport = ['sport', 'stadium', 'golf', 'basketball', 'tennis']\n",
    "music = ['music', 'jazz']\n",
    "safe = ['safe', 'secure', 'security']\n",
    "smoke = ['smoke', 'smoking']\n",
    "traffic = ['traffic', 'highway']\n",
    "pet = ['pet', 'dog']\n",
    "basic = ['basic', 'simple']\n",
    "cheap = ['cheap', 'discount', 'bonus']\n",
    "#pool = ['pool', 'swimming']\n",
    "\n",
    "lists = [view, walk, positive, pos_sent, very_pos, new, big, near, minutes, quiet, beautiful, \n",
    "         comfortable, cozy, central, many, shopping, porch, entire, couch,\n",
    "         historic, outdoors, sunny, easy, luxury, charming, near_ocean, \n",
    "         near_small_water, famous, relax, fast, modern, vacation, busy, stone, upscale,\n",
    "         architecture, vintage, culture, fresh, sport, music, safe, smoke, traffic, pet, basic,\n",
    "         cheap]\n",
    "\n",
    "words2 = pd.DataFrame()\n",
    "\n",
    "# Redo the count so that we have a single count for each group of similar words\n",
    "for item in lists:\n",
    "    x = 1\n",
    "    words2[item[0]] = words[item[0]]\n",
    "    while x < len(item):\n",
    "        words2[item[0]] += words[item[x]]\n",
    "        x += 1\n",
    "\n",
    "# rename some columns\n",
    "words2 = words2.rename(columns={'great':'positive', 'inviting':'pos_sent', \n",
    "                                'best':'very_pos', 'museum':'attractions', 'ocean':'near_ocean',\n",
    "                                'river':'near_small_water'})\n",
    "    \n",
    "\n",
    "\n",
    "keep_words = ['private', 'home', 'house', 'full', 'living',\n",
    "              'space', 'away', 'access', 'two', \n",
    "              'neighborhood', 'area', 'floor', 'beach', \n",
    "              'queen',\n",
    "              'location', 'free', 'complimentary', 'distance', 'spacious', 'clean',\n",
    "              'downtown', 'studio', 'need', 'square',\n",
    "              'coffee', 'well', 'fully', 'heart', 'pizza',\n",
    "              'love', \n",
    "              'garden', 'everything', 'light', 'small', 'high', 'west', \n",
    "              'table', 'air',\n",
    "              'closet', 'furnished', 'village', 'open', 'nice', 'stop', \n",
    "              'front', 'short', \n",
    "              'public', 'top', 'friendly', 'work', \n",
    "              'king', 'separate', \n",
    "              'suite', 'solo',  'balcony', 'local',\n",
    "              'privacy', 'basement', 'share',\n",
    "              'little', 'old', 'south', 'phone', 'hidden', 'major', 'happy',\n",
    "              'experience', 'north', 'additional',\n",
    "              'style', 'stylish', 'hotel', \n",
    "              'garage', 'furniture', 'upper', 'lower', 'cool',\n",
    "              'tree', 'unique', 'fun', 'brownstone', 'community', 'hot',\n",
    "              'double', 'ideal', 'half',\n",
    "              'duplex', 'fireplace', 'rental', 'office', 'lounge', \n",
    "              'circle', 'decorated', 'bridge', 'accessible', 'soho',\n",
    "              'terrace', 'accommodate', 'national', 'ill', 'golden', 'complete', 'cottage',\n",
    "              'island',\n",
    "              'fort', 'lax', 'warm', 'vibrant', 'classic', 'explore', 'dresser', \n",
    "              'smart', 'toaster', 'broadway', \n",
    "              'glass', 'summer', 'canyon', 'bungalow', 'extremely',\n",
    "              'anywhere', 'roommate', 'designed',\n",
    "              'airy', 'grove', 'popular', 'facing', 'courtyard', \n",
    "              'noise', 'used', 'echo', 'personal', 'however', 'cleaning',\n",
    "              'young',\n",
    "              'special',\n",
    "              'diverse', 'field', 'century',\n",
    "              'quaint', 'bunk',\n",
    "              'organic', 'far', 'burbank',\n",
    "              'boardwalk', 'eclectic', 'variety', 'club',\n",
    "              'sweet', 'concierge', 'escape',\n",
    "              'connected', \n",
    "              'chill', 'wooden',\n",
    "              'party',\n",
    "              'flexible', 'boulevard', 'nook', 'energy',\n",
    "              'professionally',\n",
    "              'gallery', 'vista', \n",
    "              'skylight', 'suitable', 'important', 'communal', 'cabin',\n",
    "              'game',\n",
    "              'tour', 'social', 'breeze',\n",
    "              'quarter', 'rustic', 'hammock', 'detached', 'alone', 'empty', \n",
    "              'rented', 'freedom',\n",
    "              'traditional', 'soft', 'standard', \n",
    "              'weekly', 'monthly', 'uptown',\n",
    "              'treat', 'underground', \n",
    "              'sorry', 'barbecue', 'parlor',\n",
    "              'finished', 'foggy',\n",
    "              'retail', 'remote',\n",
    "              'occasionally', \n",
    "              'business', 'hardwood']\n",
    "\n",
    "for i in keep_words:\n",
    "    words2[i] = words[i]\n",
    "\n",
    "# Drop any words? Library? gym, kitchen, hospital\n",
    "\n",
    "# =============================================================================\n",
    "# ############# Modeling #############################\n",
    "# =============================================================================\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmse_scorer = make_scorer(rmse, greater_is_better=True)\n",
    "\n",
    "def prep(Data):\n",
    "    test_clean = Data[Data['dataset']=='test']\n",
    "    train2 = Data[Data['dataset']=='train']\n",
    "    train2 = train2.drop(['dataset', 'id'], axis=1)\n",
    "    # split out target and predictors\n",
    "    y = train2['log_price']\n",
    "    x = train2.drop(['log_price'], axis=1)\n",
    "    return x, y, test_clean\n",
    "\n",
    "\n",
    "\n",
    "############# XGBoost #############################\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "train = data[data.dataset == \"train\"]\n",
    "test = data[data.dataset == 'test']\n",
    "\n",
    "x_test = test.drop(['log_price', 'dataset'], axis=1)\n",
    "x_test2 = test.drop(['log_price', 'dataset', 'id'], axis=1)\n",
    "\n",
    "x_train = train.drop(['log_price', 'id', 'dataset'], axis=1)\n",
    "y_train = train['log_price']\n",
    "\n",
    "dtrain = xgb.DMatrix(x_train, y_train)\n",
    "dtest = xgb.DMatrix(x_test2)\n",
    "\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': np.mean(y_train),\n",
    "    'silent': 0\n",
    "}\n",
    "\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=242)\n",
    "mode2 = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=242)\n",
    "mode3 = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=242)\n",
    "\n",
    "\n",
    "z = model.predict(dtest)\n",
    "\n",
    "z3 = mode3.predict(dtest)\n",
    "\n",
    "print('RMSE:',mean_squared_error(model.predict(dtest), y_train)**(1/2)) #0.376, #0.363\n",
    "\n",
    "submission = pd.DataFrame(np.column_stack([x_test.id, z3]), columns = ['id','log_price'])\n",
    "submission.to_csv(\"hz_submission2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Amenities Processing\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "f = lambda x : [r for r in re.sub(r'[^,a-z0-9]','',x.lower()).split(',') if len(r) > 1]\n",
    "amenities = pd.get_dummies(data['amenities'].map(f).apply(pd.Series).stack()).sum(level=0)\n",
    "\n",
    "# Loop through amenities to make sure all values are either 0 or 1. If >1, change to 1\n",
    "for col in amenities:\n",
    "    amenities.loc[(amenities[col]>1), col] = 1\n",
    "\n",
    "# Consolidate duplicate amenities\n",
    "group = {'accessible_disable': ['accessibleheightbed', 'accessibleheighttoilet'], \n",
    " 'has_elevator': ['elevator', 'elevatorinbuilding'],\n",
    " 'firm_matress': ['firmmatress', 'firmmattress'],\n",
    " 'goodpathwaytofrontdoor': ['flatsmoothpathwaytofrontdoor',  'smoothpathwaytofrontdoor'],\n",
    " 'well_lit_path': ['welllitpathtoentrance', 'pathtoentrancelitatnight'],\n",
    " 'wideclearancetoshower_and_toilet': ['wideclearancetoshowerandtoilet', 'wideclearancetoshowertoilet'],\n",
    " }\n",
    "\n",
    "        \n",
    "for g in group:\n",
    "    amenities[g] = 0\n",
    "    amenities.loc[(amenities[group[g][0]] == 1) | (amenities[group[g][1]] == 1), g] = 1\n",
    "    amenities = amenities.drop(group[g], axis=1)\n",
    "\n",
    "# instances where dog, cats, otherpets all 0 and petsliveonproperty is 1 -> make otherpets 1\n",
    "amenities[(amenities['dogs']==0) & (amenities['cats']==0) & (amenities['otherpets']==0) & (amenities['petsliveonthisproperty']==1)][['cats', 'dogs', 'otherpets', 'petsliveonthisproperty']].shape   \n",
    "amenities[((amenities['dogs']==1) | (amenities['cats']==1) | (amenities['otherpets']==1)) & (amenities['petsliveonthisproperty']==1)][['cats', 'dogs', 'otherpets', 'petsliveonthisproperty']].shape   \n",
    "\n",
    "# if  petsliveonthisproperty = 1, replace otherpets with 1 (done for 2,677 instances)\n",
    "amenities.loc[((amenities['dogs']==0) & (amenities['cats']==0) & (amenities['otherpets']==0) & (amenities['petsliveonthisproperty']==1)), 'otherpets'] = 1\n",
    "\n",
    "amenities = amenities.drop(['petsliveonthisproperty'], axis=1)\n",
    "\n",
    "\n",
    "################## Washer Dryer ###############################\n",
    "# washer, washerdryer, dryer\n",
    "\n",
    "# divide into washer and dryer -> any instances where washerdryer = 1, but washer and dryer are 0\n",
    "amenities[(amenities['washerdryer']==1) & (amenities['washer']==0) & (amenities['dryer']==0)][['washerdryer', 'washer', 'dryer']]\n",
    "# No instances\n",
    "\n",
    "#drop washerdryer\n",
    "amenities = amenities.drop(['washerdryer'], axis=1)\n",
    "\n",
    "\n",
    "# Drop other unneeded vars\n",
    "amenities = amenities.drop(['translationmissingenhostingamenity49', 'translationmissingenhostingamenity50'], axis=1)\n",
    "\n",
    "amenities =amenities.rename(columns = {'gym':'amenity_gym'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bbae47e11268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import re\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# ensure all necessary files are loaded\n",
    "train_file = os.path.join(data_path,'train.csv')\n",
    "if not os.path.isfile(train_file):\n",
    "    raise FileNotFoundError(\"Training file not found\")\n",
    "\n",
    "test_file = os.path.join(data_path,'test.csv')\n",
    "if not os.path.isfile(test_file):\n",
    "    raise FileNotFoundError(\"Testing file not found\")\n",
    "\n",
    "rgb_data_file = os.path.join(data_path, 'withRgb.csv')\n",
    "if not os.path.isfile(rgb_data_file):\n",
    "    raise FileNotFoundError(\"RGB file not found\")\n",
    "\n",
    "ct_median_income_file = os.path.join(data_path, 'ct_median_income.csv')\n",
    "if not os.path.isfile(ct_median_income_file):\n",
    "    raise FileNotFoundError(\"Median income file not found\")\n",
    "\n",
    "zillow_file = os.path.join(data_path, 'Zip_MedianRentalPrice_AllHomes.csv')\n",
    "if not os.path.isfile(zillow_file):\n",
    "    raise FileNotFoundError(\"Zillow data file not found\")\n",
    "    \n",
    "xg_boost_model_file = os.path.join(model_path, 'xg_model.dat')\n",
    "if not os.path.isfile(xg_boost_model_file):\n",
    "    raise FileNotFoundError(\"Saved XGBoost model file not found\")\n",
    "\n",
    "# load the training and test csv files\n",
    "train = pd.read_csv(train_file)\n",
    "test = pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Import Other Data\n",
    "# =============================================================================\n",
    "\n",
    "census_income = pd.read_csv('./data/ct_median_income.csv')\n",
    "census_income = census_income.drop(['Unnamed: 0'], axis=1)\n",
    "\n",
    "zillow = pd.read_csv('./data/Zip_MedianRentalPrice_AllHomes.csv',index_col='RegionName')['2017-12']zillow.index = [str(zip) for zip in zillow.index]\n",
    "\n",
    "rgb = pd.read_csv('./data/withRgb.csv',encoding='iso-8859-1')\n",
    "# =============================================================================\n",
    "# Calculate distance between city center and airbnb property\n",
    "# =============================================================================\n",
    "\n",
    "landmarks = {'Boston': {'latitude': 42.3431969605, 'longitude': -71.0726130429},  # copley\n",
    "             'NYC': {'latitude': 40.758896, 'longitude': -73.985130},  #times square\n",
    "             'SF': {'latitude': 37.8096506, 'longitude': -122.410249}, # fisherman's wharf\n",
    "             'LA': {'latitude': 34.101166262, 'longitude': -118.337915315}, # hollywood(chinese theater)\n",
    "             'DC': {'latitude': 38.897957, 'longitude': -77.036560}, # white house\n",
    "             'Chicago': {'latitude': 41.8891, 'longitude': -87.626674}  # trump tower\n",
    "        }\n",
    "\n",
    "\n",
    "data['distance_calc'] = 0 \n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    room = (row['latitude'], row['longitude'])\n",
    "    landmark = (landmarks[next(iter(set([row['city']])&set(list(landmarks.keys()))))]['latitude'], \n",
    "                          landmarks[next(iter(set([row['city']])&set(list(landmarks.keys()))))]['longitude'])\n",
    "    dist = vincenty(room, landmark).meters/1000.000\n",
    "    data.set_value(index, 'distance_calc', dist)\n",
    "    #print(index)\n",
    "    \n",
    "    \n",
    "# =============================================================================\n",
    "# Add in info from google places api\n",
    "# =============================================================================\n",
    "places = pd.read_csv('./data/latlng_edited.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Merge into dataset by latlng combination\n",
    "data['lat_round'] = round(data['latitude'], 3)\n",
    "data['lng_round'] = round(data['longitude'], 3)\n",
    "data['latlngcoords'] = data['lat_round'].map(str) + ','+ data['lng_round'].map(str)\n",
    "\n",
    "data = pd.merge(data, places, on='latlngcoords', how='left')\n",
    "\n",
    "#drop latlng, latlngcoords\n",
    "data = data.drop(['lat', 'latlngcoords', 'lng', 'lat_round', 'lng_round', 'places', 'placetype', \n",
    "                  'middle_places'], axis=1)\n",
    " \n",
    "    \n",
    "####################################################\n",
    "############# Consolidate Data ############################\n",
    "####################################################\n",
    "\n",
    "    \n",
    "data = pd.concat([data, amenities], axis=1)\n",
    "data = data.merge(rgb[['id','meanG','meanR','meanB']],left_on='id',right_on='id')\n",
    "data = data.merge(census_income[['id','ct_median_income']],left_on='id',right_on='id')\n",
    "data['ct_median_income'] = pd.to_numeric(data['ct_median_income'])\n",
    "data['home_prices_zillow'] = data['zipcode'].map(zillow)\n",
    "\n",
    "####################################################\n",
    "############# Further Processing ############################\n",
    "####################################################\n",
    "\n",
    "# =============================================================================\n",
    "# Data PreProcessing after merging\n",
    "# =============================================================================\n",
    "'''Property Type'''\n",
    "\n",
    "data['property_type'] = data['property_type'].replace(['Bed & Breakfast', 'Bungalow', 'Villa', 'Guest suite'], 'Guesthouse')\n",
    "data['property_type'] = data['property_type'].replace(['Dorm', 'Hut', 'Treehouse'], 'Other')\n",
    "\n",
    "data['property_type'] = data['property_type'].replace(['Camper/RV', 'Timeshare', 'Cabin', 'Hostel', 'In-law', \n",
    "    'Boutique hotel', 'Boat', 'Serviced apartment', 'Tent', 'Castle', 'Vacation home', 'Yurt', \n",
    "    'Chalet', 'Earth House', 'Tipi', 'Train', 'Cave', 'Parking Space', \n",
    "    'Casa particular', 'Lighthouse', 'Island'], 'Other2')\n",
    "\n",
    "### Dates\n",
    "data['first_review'] = pd.to_datetime(data['first_review'])\n",
    "data['last_review'] = pd.to_datetime(data['last_review'])\n",
    "data['host_since'] = pd.to_datetime(data['host_since'])\n",
    "\n",
    "data['now'] = datetime.datetime.now()\n",
    "data['num_days_since_last_review'] = data['now'] - data['last_review']\n",
    "\n",
    "data['num_days_since_last_review'] = [x.days for x in data['num_days_since_last_review']]\n",
    "\n",
    "data['host_length'] = [x.days for x in (data['now'] - data['host_since'])]\n",
    "\n",
    "data['host_response_rate'] = data['host_response_rate'].map(lambda x: float(x.split('%')[0])/100 if isinstance(x,str) else 0)\n",
    "\n",
    "\n",
    "############# Map Dummy Vars #########################\n",
    "data['instant_bookable'] = data['instant_bookable'].map({'f':0,'t':1})\n",
    "data['host_has_profile_pic'] = data['host_has_profile_pic'].map({'f':0,'t':1})\n",
    "data['host_identity_verified'] = data['host_identity_verified'].map({'f':0,'t':1})\n",
    "data['cleaning_fee'] = data['cleaning_fee'].map({False:0,True:1})\n",
    "\n",
    "############# Check Missing Vals  ###############################\n",
    "\n",
    "# replace by 1 if value missing:\n",
    "# bathrooms, bedrooms, beds\n",
    "data.update(data[['bathrooms', 'bedrooms', 'beds']].fillna(1))\n",
    "\n",
    "#data = data.reset_index()\n",
    "# replace by 0 if missing:\n",
    "# host_has_profile_pic, host_identity_verified, all amenities\n",
    "data.update(data[(['host_has_profile_pic', 'host_identity_verified']+amenities.columns.tolist())].fillna(0))\n",
    "#data.update(data[(['host_has_profile_pic', 'host_identity_verified'])].fillna(0))\n",
    "#\n",
    "#for x in amenities:\n",
    "#    data[x].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "# replace by median: review_scores_rating, meanG, meanR, meanB, ct_median_income, host_length\n",
    "replace_median = ['review_scores_rating', 'meanG', 'meanR', 'meanB', 'ct_median_income', 'host_length', 'num_days_since_last_review']\n",
    "for i in replace_median:\n",
    "    data[i].fillna(data[i].median(), inplace=True)\n",
    "\n",
    "# neighborhood missing 9K values\n",
    "# replace by city if neighborhood missing\n",
    "data['neighbourhood'].fillna(data['city'], inplace=True)\n",
    "\n",
    "############# Encode ###############################\n",
    "data = pd.get_dummies(data, columns=['property_type', 'room_type', 'bed_type', \n",
    "                                     'cancellation_policy', 'neighbourhood', 'city'])\n",
    "    \n",
    "\n",
    "############# Drop Vars ###############################\n",
    "data = data.drop(['index', 'amenities', 'description', 'first_review', 'host_since', \n",
    "                    'last_review', 'name', 'thumbnail_url', \n",
    "                    'zipcode', 'now', 'latitude', 'longitude'], axis=1)\n",
    "\n",
    "############# Add in  ###############################\n",
    "data = pd.concat([data, words2], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-002122a2ff06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m#load pickled xgboost model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxg_boost_model_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# One-hot-encode categorical variables\n",
    "train['dataset'] = \"train\"\n",
    "test['dataset'] = \"test\"\n",
    "data = pd.concat([train,test], axis = 0).reset_index()\n",
    "categorical = ['property_type','room_type','bed_type','cancellation_policy','city']\n",
    "data = pd.get_dummies(data, columns = categorical)\n",
    "\n",
    "#Function to convert amentities string to list\n",
    "f = lambda x : [r for r in re.sub(r'[^,a-z0-9]','',x.lower()).split(',') if len(r) > 1]\n",
    "#Amenities list to dummy vars\n",
    "amenities = pd.get_dummies(data['amenities'].map(f).apply(pd.Series).stack()).sum(level=0)\n",
    "data = pd.concat([data,amenities],axis=1)\n",
    "\n",
    "\n",
    "##Some extra features to create from base data\n",
    "data['host_response_rate'] = data['host_response_rate'].map(lambda x: float(x.split('%')[0])/100 if isinstance(x,str) else 0)\n",
    "data['instant_bookable'] = data['instant_bookable'].map({'f':0,'t':1})\n",
    "data['host_has_profile_pic'] = data['host_has_profile_pic'].map({'f':0,'t':1})\n",
    "data['cleaning_fee'] = data['cleaning_fee'].map({False:0,True:1})\n",
    "\n",
    "\n",
    "#add rgb data to dataset\n",
    "rgb = pd.read_csv(rgb_data_file, encoding='iso-8859-1')\n",
    "\n",
    "data = data.merge(rgb[['id','meanG','meanR','meanB']],left_on='id',right_on='id')\n",
    "\n",
    "#add median income for census tract to dataset\n",
    "\n",
    "ct_median_income = pd.read_csv(ct_median_income_file)\n",
    "data = data.merge(ct_median_income[['id','ct_median_income']],left_on='id',right_on='id')\n",
    "data['ct_median_income'] = pd.to_numeric(data['ct_median_income'])\n",
    "\n",
    "#add zillow data to dataset\n",
    "zillow = pd.read_csv(zillow_file,index_col='RegionName')['2017-12']\n",
    "zillow.index = [str(zip) for zip in zillow.index]\n",
    "data['home_prices_zillow'] = data['zipcode'].map(zillow)\n",
    "\n",
    "\n",
    "#load pickled xgboost model\n",
    "model = pickle.load(open(xg_boost_model_file,'rb'))\n",
    "\n",
    "numerics = ['uint8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "train_x = data[data.dataset == \"train\"] \\\n",
    "    .select_dtypes(include=numerics) \\\n",
    "    .drop(\"log_price\", axis = 1) \\\n",
    "    .fillna(0) \\\n",
    "    .values\n",
    "\n",
    "\n",
    "test_x = data[data.dataset == \"test\"] \\\n",
    "    .select_dtypes(include=numerics) \\\n",
    "    .drop(\"log_price\", axis = 1) \\\n",
    "    .fillna(0) \\\n",
    "    .values\n",
    "    \n",
    "train_y = data[data.dataset == \"train\"].log_price.values\n",
    "\n",
    "dtrain = xgb.DMatrix(train_x, train_y)\n",
    "\n",
    "print('RMSE:',mean_squared_error(model.predict(dtrain), train_y )**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Best xgboost model so far\n",
    "\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "numerics = ['uint8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "train_x = data[data.dataset == \"train\"] \\\n",
    "    .select_dtypes(include=numerics) \\\n",
    "    .drop(\"log_price\", axis = 1) \\\n",
    "    .fillna(0) \\\n",
    "    .values\n",
    "\n",
    "\n",
    "test_x = data[data.dataset == \"test\"] \\\n",
    "    .select_dtypes(include=numerics) \\\n",
    "    .drop(\"log_price\", axis = 1) \\\n",
    "    .fillna(0) \\\n",
    "    .values\n",
    "    \n",
    "train_y = data[data.dataset == \"train\"].log_price.values\n",
    "\n",
    "\n",
    "y_mean = np.mean(train_y)\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(train_x, train_y)\n",
    "dtest = xgb.DMatrix(test_x)\n",
    "\n",
    "# cross-validation\n",
    "#print( \"Running XGBoost CV ...\" )\n",
    "#cv_result = xgb.cv(xgb_params, \n",
    "#                   dtrain, \n",
    "#                   nfold=5,\n",
    "#                   num_boost_round=350,\n",
    "#                   early_stopping_rounds=50,\n",
    "#                   verbose_eval=10, \n",
    "#                   show_stdv=False\n",
    "#                  )\n",
    "#num_boost_rounds = len(cv_result)\n",
    "\n",
    "# num_boost_rounds = 150\n",
    "num_boost_rounds = 242\n",
    "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n",
    "\n",
    "# train model\n",
    "print( \"\\nTraining XGBoost ...\")\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "#Pickle model object\n",
    "pickle.dump(model,open('xg_model.dat','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##NLP - not currently used \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "##NLP STUFF HERE\n",
    "\n",
    "tokens = []\n",
    "bigrams = []\n",
    "\n",
    "#Find top words and bigrams (2 word pairs) from the listing names\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for name in data['name']:\n",
    "    if isinstance(name,str):\n",
    "        words = [word for word in nltk.word_tokenize(re.sub(r'[^ a-z]','',name.lower())) if word not in stop_words]\n",
    "        bigrams.extend(nltk.bigrams(words))\n",
    "        tokens.extend(words)\n",
    "\n",
    "#Count occurences of top bigrams and words in listing name\n",
    "topBigrams = [' '.join(b[0]) for b in Counter(bigrams).most_common(25)]\n",
    "topWords = [w[0] for w in Counter(tokens).most_common(25)]\n",
    "countTopWords = lambda x: len(set(re.sub(r'[^ a-z]','',x.lower()).split()).intersection(set(topWords))) if isinstance(x,str) else 0\n",
    "countTopBigrams = lambda x: sum([re.sub(r'[^ a-z]','',x.lower()).count(bigram) for bigram in topBigrams]) if isinstance(x,str) else 0\n",
    "\n",
    "#use weight of topwords and topbirgrams - for example 25 would be 1 point, 0 would be 25 points\n",
    "\n",
    "\n",
    "data['num_top_words'] = data['name'].map(countTopWords)\n",
    "data['num_top_bigrams'] = data['name'].map(countTopBigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "\n",
    "##RGB Analysis - see image download script below\n",
    "##BEWARE: Takes a long time to get mean RGB for dataset\n",
    "\n",
    "def getBrightnessForImage(id=None,url=None,useImageData=True):\n",
    "    if useImageData:\n",
    "        path = 'images/{}.jpg'.format(id)\n",
    "        if os.path.exists(path):\n",
    "            image = Image.open(path)\n",
    "        else:\n",
    "            return(row)\n",
    "    else:\n",
    "        image = Image.open(requests.get(url,stream=True).raw)\n",
    "    RGBs = []\n",
    "    for x in range(image.width):\n",
    "        for y in range(image.height):\n",
    "            RGBs.append(sum(image.getpixel((x,y)))/3)\n",
    "    return(sum(RGBs)/len(RGBs))   \n",
    "\n",
    "def getRGBForImage(row,useImageData=True):\n",
    "    \n",
    "    if useImageData:\n",
    "        id = row['id']\n",
    "        path = 'images/{}.jpg'.format(id)\n",
    "        if os.path.exists(path):\n",
    "            image = Image.open(path)\n",
    "        else:\n",
    "            return(row)\n",
    "    else:\n",
    "        url = row['thumnail_url']\n",
    "        image = Image.open(requests.get(url,stream=True).raw)\n",
    "    RGBs = []\n",
    "    for x in range(image.width):\n",
    "        for y in range(image.height):\n",
    "            rgb = image.getpixel((x,y)) \n",
    "            RGBs.append(rgb)\n",
    "    transposed = np.array(RGBs).T\n",
    "    meanR, meanG, meanB = np.mean(transposed[0]),np.mean(transposed[1]),np.mean(transposed[2])\n",
    "    \n",
    "    row['meanR'] = meanR\n",
    "    row['meanG'] = meanG\n",
    "    row['meanB'] = meanB\n",
    "    \n",
    "    return(row)\n",
    "\n",
    "\n",
    "data = data.apply(getRGBForImage,axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import shapefile\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from shapely.geometry import shape as Shape, Point\n",
    "\n",
    "###Example of spatial feature extraction\n",
    "###Finds the median household income of the Census tract from listing lng/lat \n",
    "###Takes a while to reload data\n",
    "\n",
    "\n",
    "#Scrape zip file links to dict {stateName:zipFileURL} for census tract shape files\n",
    "censusShapeURL = 'https://www.census.gov/geo/maps-data/data/cbf/cbf_tracts.html'\n",
    "soup = BeautifulSoup(requests.get(censusShapeURL).content,'lxml')\n",
    "ctShapefiles = {option.text.strip():option.get('value') for option in soup.find(id='ct2016m').findAll('option')}\n",
    "\n",
    "#Could replace this with a less hard-coded method\n",
    "cityToState = {'NYC':'New York', \n",
    "               'SF':'California', \n",
    "               'DC':'District of Columbia', \n",
    "               'LA': 'California', \n",
    "               'Chicago': 'Illinois', \n",
    "               'Boston': 'Massachusetts'}\n",
    "\n",
    "ctDict = {}\n",
    "#Download all shapefiles needed, unzip, and add to a shapefile dict {censusTract:shapeFile}\n",
    "for zipFile in data.city.map(cityToState).map(ctShapefiles).unique():\n",
    "    shapefilePath = './data/census_tract_shapefiles/{}.shp'.format(zipFile.split('/')[-1].split('.')[0])\n",
    "    if os.path.exists(shapefilePath): \n",
    "        print('{} already exists, using local copy'.format(shapefilePath))\n",
    "    else:\n",
    "        print('downloading {} ... '.format(zipFile))\n",
    "        r = requests.get(zipFile)\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall('./data/census_tract_shapefiles')\n",
    "        \n",
    "    shape = shapefile.Reader(shapefilePath)\n",
    "    ctDict.update({ feature.record[3]: Shape(feature.shape) for feature in shape.shapeRecords()})\n",
    "  \n",
    "#Function that will return the census tract for given coordinates\n",
    "def getFeatureforPoint(shapeDict,lng,lat):\n",
    "    point = Point(lng,lat)\n",
    "    for feature, shape in shapeDict.items():\n",
    "        if shape.contains(point):\n",
    "            return(feature)\n",
    "\n",
    "        \n",
    "\n",
    "#Loop through and save census data for each state \n",
    "ctIncomePath = './data/ct_income.csv'\n",
    "if os.path.exists(ctIncomePath): \n",
    "    print('{} already exists, using local copy'.format(ctIncomePath))\n",
    "    pd.read_csv(ctIncomePath)\n",
    "else:\n",
    "    ctIncome = pd.DataFrame()\n",
    "    for i in range(1,57):\n",
    "        url = 'https://api.census.gov/data/2016/acs/acs5?get=NAME,B19013_001E&for=tract:*&in=state:{}'.format(str(i).zfill(2))\n",
    "        try:\n",
    "            df = pd.DataFrame(requests.get(url).json())\n",
    "        except:\n",
    "            next\n",
    "\n",
    "        ctIncome = pd.concat([ctIncome,df])\n",
    "\n",
    "    #return series of tract to median income\n",
    "    ctIncome.columns = ctIncome.iloc[0]\n",
    "    ctIncome = ctIncome.drop_duplicates().iloc[1:]\n",
    "    ctIncome['full_tract_name'] = '1400000US' + ct_income['state'] + ct_income['county'] + ct_income['tract']\n",
    "    ctIncome = ctIncome.set_index('full_tract_name')['B19013_001E'] \n",
    "    ctIncome.to_csv(ctIncomePath)\n",
    "\n",
    "##Should store census tract locations to id mapping also\n",
    "data.apply(lambda row:\\\n",
    "           getFeatureforPoint(ctDict,row['longitude'],row['latitude']),axis = 1)\\\n",
    "           .map(ctIncome)\n",
    "    \n",
    "data[['id','ct_median_income']].to_csv('./data/features/ct_median_income.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Keras test, not being used\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "path = \"\"\n",
    "train = pd.read_csv(path + \"train.csv\")\n",
    "test = pd.read_csv(path + \"test.csv\")\n",
    "\n",
    "# One-hot-encode categorical variables\n",
    "train['dataset'] = \"train\"\n",
    "test['dataset'] = \"test\"\n",
    "data = pd.concat([train,test], axis = 0)\n",
    "categorical = ['property_type','room_type','bed_type','cancellation_policy','city']\n",
    "data = pd.get_dummies(data, columns = categorical)\n",
    "\n",
    "\n",
    "\n",
    "# Select only numeric data and impute missing values as 0\n",
    "numerics = ['uint8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(len(train_x[0]), input_dim=len(train_x[0]), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "#seed = 7\n",
    "#np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "#estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=0)\n",
    "\n",
    "#kfold = KFold(n_splits=10, random_state=seed)\n",
    "#results = cross_val_score(estimator, train_x, train_y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#xgboost test\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.datasets import load_iris, load_digits, load_boston\n",
    "\n",
    "\n",
    "numerics = ['uint8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "train_x = data[data.dataset == \"train\"] \\\n",
    "    .select_dtypes(include=numerics) \\\n",
    "    .drop(\"log_price\", axis = 1) \\\n",
    "    .fillna(0) \\\n",
    "    .values\n",
    "\n",
    "\t\n",
    "test_x = data[data.dataset == \"test\"] \\\n",
    "    .select_dtypes(include=numerics) \\\n",
    "    .drop(\"log_price\", axis = 1) \\\n",
    "    .fillna(0) \\\n",
    "    .values\n",
    "    \n",
    "train_y = data[data.dataset == \"train\"].log_price.values\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(31337)\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=rng)\n",
    "for train_index, test_index in kf.split(train_x):\n",
    "    xgb_model = xgb.XGBRegressor().fit(train_x[train_index], train_y[train_index])\n",
    "    predictions = xgb_model.predict(train_x[test_index])\n",
    "    actuals = train_y[test_index]\n",
    "    print(mean_squared_error(actuals, predictions))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Attempting to create a model based on iterations\n",
    "xgb_params.update({\n",
    "            'learning_rate': 0.007,\n",
    "            'update':'refresh',\n",
    "           # 'process_type': 'update',\n",
    "            'refresh_leaf': True,\n",
    "            #'reg_lambda': 3,  # L2\n",
    "            'reg_alpha': 3,  # L1\n",
    "            'silent': False,\n",
    "        })\n",
    "batch_size = 5000\n",
    "iterations = 10\n",
    "model = None\n",
    "for i in range(iterations):\n",
    "    for start in range(0, len(train_x), batch_size):\n",
    "        print('batch..')\n",
    "        model = xgb.train(xgb_params, num_boost_round=150, dtrain=xgb.DMatrix(train_x[start:start+batch_size], train_y[start:start+batch_size]), xgb_model=model)\n",
    "\n",
    "        y_pr = model.predict(xgb.DMatrix(train_x))\n",
    "        #print('    MSE itr@{}: {}'.format(int(start/batch_size), sklearn.metrics.mean_squared_error(y_te, y_pr)))\n",
    "    print('MSE itr@{}: {}'.format(i, mean_squared_error(train_y, y_pr)))\n",
    "\n",
    "#y_pr = model.predict(xgb.DMatrix(x_te))\n",
    "#print('MSE at the end: {}'.format(mean_squared_error(test_y, y_pr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Attempt to find optimum depth, child weight (never finished processing)\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "\n",
    "param_test1 =   {\n",
    " 'max_depth':list(range(3,10,2)),\n",
    " 'min_child_weight':list(range(1,6,2))\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(train_x,train_y)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_prediction = model.predict(dtest)\n",
    "submission = pd.DataFrame(np.column_stack([test.id, final_prediction]), columns = ['id','log_price'])\n",
    "submission.to_csv(\"fourth_submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Download images\n",
    "\n",
    "import os \n",
    "import requests\n",
    "\n",
    "\n",
    "for i,row in test.iterrows():  \n",
    "    if isinstance(row['thumbnail_url'],str) and not os.path.exists('./images/{}.jpg'.format(row['id'])):\n",
    "        print(i)\n",
    "        url = row['thumbnail_url']\n",
    "        filename = './images/{}.jpg'.format(row['id'])\n",
    "        try:\n",
    "            r = requests.get(url, timeout=1.5)\n",
    "            if r.status_code == 200:\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "        except:\n",
    "            print('timeout')\n",
    "            next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Correlation matrix\n",
    "\n",
    "%matplotlib inline\n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "train\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = train.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##With random forest \n",
    "\n",
    "#from test script\n",
    "0.16969142613\n",
    "0.172199948847\n",
    "0.171665702104\n",
    "\n",
    "#with NLP \n",
    "0.170133091601\n",
    "0.172209588905\n",
    "0.171824024348\n",
    "\n",
    "\n",
    "#with amenites\n",
    "0.203165992454\n",
    "0.205297239503\n",
    "0.201059075764\n",
    "\n",
    "\n",
    "#with RGB\n",
    "0.169998647333\n",
    "0.171589382807\n",
    "0.171464521269\n",
    "\n",
    "\n",
    "#with census median income\n",
    "0.167652666241\n",
    "0.169194518538\n",
    "0.169065016674\n",
    "\n",
    "#with host response rate\n",
    "0.166819384931\n",
    "0.1681722164\n",
    "0.168567149508\n",
    "\n",
    "#with instant bookable\n",
    "0.166553242784\n",
    "0.168167160006\n",
    "0.168633984499\n",
    "\n",
    "0.166422024511\n",
    "0.168085883256\n",
    "0.168238200829\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
