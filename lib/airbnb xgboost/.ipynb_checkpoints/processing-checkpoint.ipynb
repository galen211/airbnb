{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "path = \"\"\n",
    "train = pd.read_csv(path + \"train.csv\")\n",
    "test = pd.read_csv(path + \"test.csv\")\n",
    "\n",
    "# One-hot-encode categorical variables\n",
    "train['dataset'] = \"train\"\n",
    "test['dataset'] = \"test\"\n",
    "data = pd.concat([train,test], axis = 0).reset_index()\n",
    "categorical = ['property_type','room_type','bed_type','cancellation_policy','city']\n",
    "data = pd.get_dummies(data, columns = categorical)\n",
    "\n",
    "#Function to convert amentities string to list\n",
    "f = lambda x : [r for r in re.sub(r'[^,a-z0-9]','',x.lower()).split(',') if len(r) > 1]\n",
    "#Amenities list to dummy vars\n",
    "amenities = pd.get_dummies(data['amenities'].map(f).apply(pd.Series).stack()).sum(level=0)\n",
    "data = pd.concat([data,amenities],axis=1)\n",
    "\n",
    "\n",
    "##Some extra features to create from base data\n",
    "data['host_response_rate'] = data['host_response_rate'].map(lambda x: float(x.split('%')[0])/100 if isinstance(x,str) else 0)\n",
    "data['instant_bookable'] = data['instant_bookable'].map({'f':0,'t':1})\n",
    "data['host_has_profile_pic'] = data['host_has_profile_pic'].map({'f':0,'t':1})\n",
    "data['cleaning_fee'] = data['cleaning_fee'].map({False:0,True:1})\n",
    "\n",
    "\n",
    "#add rgb data to dataset\n",
    "rgb = pd.read_csv('./data/withRgb.csv',encoding='iso-8859-1')\n",
    "data = data.merge(rgb[['id','meanG','meanR','meanB']],left_on='id',right_on='id')\n",
    "\n",
    "#add median income for census tract to dataset\n",
    "\n",
    "ct_median_income = pd.read_csv('./data/ct_median_income.csv')\n",
    "data = data.merge(ct_median_income[['id','ct_median_income']],left_on='id',right_on='id')\n",
    "data['ct_median_income'] = pd.to_numeric(data['ct_median_income'])\n",
    "\n",
    "#add zillow data to dataset\n",
    "zillow = pd.read_csv('./data/Zip_MedianRentalPrice_AllHomes.csv',index_col='RegionName')['2017-12']\n",
    "zillow.index = [str(zip) for zip in zillow.index]\n",
    "data['home_prices_zillow'] = data['zipcode'].map(zillow)\n",
    "\n",
    "\n",
    "#load pickled xgboost model\n",
    "model = pickle.load(open('./models/xg_model.dat','rb'))\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "numerics = ['uint8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "train_x = data[data.dataset == \"train\"] \\\n",
    "    .select_dtypes(include=numerics) \\\n",
    "    .drop(\"log_price\", axis = 1) \\\n",
    "    .fillna(0) \\\n",
    "    .values\n",
    "\n",
    "\n",
    "test_x = data[data.dataset == \"test\"] \\\n",
    "    .select_dtypes(include=numerics) \\\n",
    "    .drop(\"log_price\", axis = 1) \\\n",
    "    .fillna(0) \\\n",
    "    .values\n",
    "    \n",
    "train_y = data[data.dataset == \"train\"].log_price.values\n",
    "\n",
    "dtrain = xgb.DMatrix(train_x, train_y)\n",
    "\n",
    "print('RMSE:',mean_squared_error(model.predict(dtrain), train_y )**(1/2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Best xgboost model so far\n",
    "\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "\n",
    "numerics = ['uint8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "train_x = data[data.dataset == \"train\"] \\\n",
    "    .select_dtypes(include=numerics) \\\n",
    "    .drop(\"log_price\", axis = 1) \\\n",
    "    .fillna(0) \\\n",
    "    .values\n",
    "\n",
    "\n",
    "test_x = data[data.dataset == \"test\"] \\\n",
    "    .select_dtypes(include=numerics) \\\n",
    "    .drop(\"log_price\", axis = 1) \\\n",
    "    .fillna(0) \\\n",
    "    .values\n",
    "    \n",
    "train_y = data[data.dataset == \"train\"].log_price.values\n",
    "\n",
    "\n",
    "y_mean = np.mean(train_y)\n",
    "\n",
    "xgb_params = {\n",
    "    'eta': 0.037,\n",
    "    'max_depth': 10,\n",
    "    'subsample': 0.80,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'mae',\n",
    "    'lambda': 0.8,   \n",
    "    'alpha': 0.4, \n",
    "    'base_score': y_mean,\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(train_x, train_y)\n",
    "dtest = xgb.DMatrix(test_x)\n",
    "\n",
    "# cross-validation\n",
    "#print( \"Running XGBoost CV ...\" )\n",
    "#cv_result = xgb.cv(xgb_params, \n",
    "#                   dtrain, \n",
    "#                   nfold=5,\n",
    "#                   num_boost_round=350,\n",
    "#                   early_stopping_rounds=50,\n",
    "#                   verbose_eval=10, \n",
    "#                   show_stdv=False\n",
    "#                  )\n",
    "#num_boost_rounds = len(cv_result)\n",
    "\n",
    "# num_boost_rounds = 150\n",
    "num_boost_rounds = 242\n",
    "print(\"num_boost_rounds=\"+str(num_boost_rounds))\n",
    "\n",
    "# train model\n",
    "print( \"\\nTraining XGBoost ...\")\n",
    "model = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n",
    "\n",
    "#Pickle model object\n",
    "pickle.dump(model,open('xg_model.dat','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##NLP - not currently used \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "##NLP STUFF HERE\n",
    "\n",
    "tokens = []\n",
    "bigrams = []\n",
    "\n",
    "#Find top words and bigrams (2 word pairs) from the listing names\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for name in data['name']:\n",
    "    if isinstance(name,str):\n",
    "        words = [word for word in nltk.word_tokenize(re.sub(r'[^ a-z]','',name.lower())) if word not in stop_words]\n",
    "        bigrams.extend(nltk.bigrams(words))\n",
    "        tokens.extend(words)\n",
    "\n",
    "#Count occurences of top bigrams and words in listing name\n",
    "topBigrams = [' '.join(b[0]) for b in Counter(bigrams).most_common(25)]\n",
    "topWords = [w[0] for w in Counter(tokens).most_common(25)]\n",
    "countTopWords = lambda x: len(set(re.sub(r'[^ a-z]','',x.lower()).split()).intersection(set(topWords))) if isinstance(x,str) else 0\n",
    "countTopBigrams = lambda x: sum([re.sub(r'[^ a-z]','',x.lower()).count(bigram) for bigram in topBigrams]) if isinstance(x,str) else 0\n",
    "\n",
    "#use weight of topwords and topbirgrams - for example 25 would be 1 point, 0 would be 25 points\n",
    "\n",
    "\n",
    "data['num_top_words'] = data['name'].map(countTopWords)\n",
    "data['num_top_bigrams'] = data['name'].map(countTopBigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import os\n",
    "\n",
    "##RGB Analysis - see image download script below\n",
    "##BEWARE: Takes a long time to get mean RGB for dataset\n",
    "\n",
    "def getBrightnessForImage(id=None,url=None,useImageData=True):\n",
    "    if useImageData:\n",
    "        path = 'images/{}.jpg'.format(id)\n",
    "        if os.path.exists(path):\n",
    "            image = Image.open(path)\n",
    "        else:\n",
    "            return(row)\n",
    "    else:\n",
    "        image = Image.open(requests.get(url,stream=True).raw)\n",
    "    RGBs = []\n",
    "    for x in range(image.width):\n",
    "        for y in range(image.height):\n",
    "            RGBs.append(sum(image.getpixel((x,y)))/3)\n",
    "    return(sum(RGBs)/len(RGBs))   \n",
    "\n",
    "def getRGBForImage(row,useImageData=True):\n",
    "    \n",
    "    if useImageData:\n",
    "        id = row['id']\n",
    "        path = 'images/{}.jpg'.format(id)\n",
    "        if os.path.exists(path):\n",
    "            image = Image.open(path)\n",
    "        else:\n",
    "            return(row)\n",
    "    else:\n",
    "        url = row['thumnail_url']\n",
    "        image = Image.open(requests.get(url,stream=True).raw)\n",
    "    RGBs = []\n",
    "    for x in range(image.width):\n",
    "        for y in range(image.height):\n",
    "            rgb = image.getpixel((x,y)) \n",
    "            RGBs.append(rgb)\n",
    "    transposed = np.array(RGBs).T\n",
    "    meanR, meanG, meanB = np.mean(transposed[0]),np.mean(transposed[1]),np.mean(transposed[2])\n",
    "    \n",
    "    row['meanR'] = meanR\n",
    "    row['meanG'] = meanG\n",
    "    row['meanB'] = meanB\n",
    "    \n",
    "    return(row)\n",
    "\n",
    "\n",
    "data = data.apply(getRGBForImage,axis=1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import shapefile\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from shapely.geometry import shape as Shape, Point\n",
    "\n",
    "###Example of spatial feature extraction\n",
    "###Finds the median household income of the Census tract from listing lng/lat \n",
    "###Takes a while to reload data\n",
    "\n",
    "\n",
    "#Scrape zip file links to dict {stateName:zipFileURL} for census tract shape files\n",
    "censusShapeURL = 'https://www.census.gov/geo/maps-data/data/cbf/cbf_tracts.html'\n",
    "soup = BeautifulSoup(requests.get(censusShapeURL).content,'lxml')\n",
    "ctShapefiles = {option.text.strip():option.get('value') for option in soup.find(id='ct2016m').findAll('option')}\n",
    "\n",
    "#Could replace this with a less hard-coded method\n",
    "cityToState = {'NYC':'New York', \n",
    "               'SF':'California', \n",
    "               'DC':'District of Columbia', \n",
    "               'LA': 'California', \n",
    "               'Chicago': 'Illinois', \n",
    "               'Boston': 'Massachusetts'}\n",
    "\n",
    "ctDict = {}\n",
    "#Download all shapefiles needed, unzip, and add to a shapefile dict {censusTract:shapeFile}\n",
    "for zipFile in data.city.map(cityToState).map(ctShapefiles).unique():\n",
    "    shapefilePath = './data/census_tract_shapefiles/{}.shp'.format(zipFile.split('/')[-1].split('.')[0])\n",
    "    if os.path.exists(shapefilePath): \n",
    "        print('{} already exists, using local copy'.format(shapefilePath))\n",
    "    else:\n",
    "        print('downloading {} ... '.format(zipFile))\n",
    "        r = requests.get(zipFile)\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall('./data/census_tract_shapefiles')\n",
    "        \n",
    "    shape = shapefile.Reader(shapefilePath)\n",
    "    ctDict.update({ feature.record[3]: Shape(feature.shape) for feature in shape.shapeRecords()})\n",
    "  \n",
    "#Function that will return the census tract for given coordinates\n",
    "def getFeatureforPoint(shapeDict,lng,lat):\n",
    "    point = Point(lng,lat)\n",
    "    for feature, shape in shapeDict.items():\n",
    "        if shape.contains(point):\n",
    "            return(feature)\n",
    "\n",
    "        \n",
    "\n",
    "#Loop through and save census data for each state \n",
    "ctIncomePath = './data/ct_income.csv'\n",
    "if os.path.exists(ctIncomePath): \n",
    "    print('{} already exists, using local copy'.format(ctIncomePath))\n",
    "    pd.read_csv(ctIncomePath)\n",
    "else:\n",
    "    ctIncome = pd.DataFrame()\n",
    "    for i in range(1,57):\n",
    "        url = 'https://api.census.gov/data/2016/acs/acs5?get=NAME,B19013_001E&for=tract:*&in=state:{}'.format(str(i).zfill(2))\n",
    "        try:\n",
    "            df = pd.DataFrame(requests.get(url).json())\n",
    "        except:\n",
    "            next\n",
    "\n",
    "        ctIncome = pd.concat([ctIncome,df])\n",
    "\n",
    "    #return series of tract to median income\n",
    "    ctIncome.columns = ctIncome.iloc[0]\n",
    "    ctIncome = ctIncome.drop_duplicates().iloc[1:]\n",
    "    ctIncome['full_tract_name'] = '1400000US' + ct_income['state'] + ct_income['county'] + ct_income['tract']\n",
    "    ctIncome = ctIncome.set_index('full_tract_name')['B19013_001E'] \n",
    "    ctIncome.to_csv(ctIncomePath)\n",
    "\n",
    "##Should store census tract locations to id mapping also\n",
    "data.apply(lambda row:\\\n",
    "           getFeatureforPoint(ctDict,row['longitude'],row['latitude']),axis = 1)\\\n",
    "           .map(ctIncome)\n",
    "    \n",
    "data[['id','ct_median_income']].to_csv('./data/features/ct_median_income.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Keras test, not being used\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "path = \"\"\n",
    "train = pd.read_csv(path + \"train.csv\")\n",
    "test = pd.read_csv(path + \"test.csv\")\n",
    "\n",
    "# One-hot-encode categorical variables\n",
    "train['dataset'] = \"train\"\n",
    "test['dataset'] = \"test\"\n",
    "data = pd.concat([train,test], axis = 0)\n",
    "categorical = ['property_type','room_type','bed_type','cancellation_policy','city']\n",
    "data = pd.get_dummies(data, columns = categorical)\n",
    "\n",
    "\n",
    "\n",
    "# Select only numeric data and impute missing values as 0\n",
    "numerics = ['uint8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "\n",
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(len(train_x[0]), input_dim=len(train_x[0]), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "#seed = 7\n",
    "#np.random.seed(seed)\n",
    "# evaluate model with standardized dataset\n",
    "#estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=0)\n",
    "\n",
    "#kfold = KFold(n_splits=10, random_state=seed)\n",
    "#results = cross_val_score(estimator, train_x, train_y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#xgboost test\n",
    "import xgboost as xgb\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.datasets import load_iris, load_digits, load_boston\n",
    "\n",
    "\n",
    "numerics = ['uint8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "train_x = data[data.dataset == \"train\"] \\\n",
    "    .select_dtypes(include=numerics) \\\n",
    "    .drop(\"log_price\", axis = 1) \\\n",
    "    .fillna(0) \\\n",
    "    .values\n",
    "\n",
    "\t\n",
    "test_x = data[data.dataset == \"test\"] \\\n",
    "    .select_dtypes(include=numerics) \\\n",
    "    .drop(\"log_price\", axis = 1) \\\n",
    "    .fillna(0) \\\n",
    "    .values\n",
    "    \n",
    "train_y = data[data.dataset == \"train\"].log_price.values\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(31337)\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=rng)\n",
    "for train_index, test_index in kf.split(train_x):\n",
    "    xgb_model = xgb.XGBRegressor().fit(train_x[train_index], train_y[train_index])\n",
    "    predictions = xgb_model.predict(train_x[test_index])\n",
    "    actuals = train_y[test_index]\n",
    "    print(mean_squared_error(actuals, predictions))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Attempting to create a model based on iterations\n",
    "xgb_params.update({\n",
    "            'learning_rate': 0.007,\n",
    "            'update':'refresh',\n",
    "           # 'process_type': 'update',\n",
    "            'refresh_leaf': True,\n",
    "            #'reg_lambda': 3,  # L2\n",
    "            'reg_alpha': 3,  # L1\n",
    "            'silent': False,\n",
    "        })\n",
    "batch_size = 5000\n",
    "iterations = 10\n",
    "model = None\n",
    "for i in range(iterations):\n",
    "    for start in range(0, len(train_x), batch_size):\n",
    "        print('batch..')\n",
    "        model = xgb.train(xgb_params, num_boost_round=150, dtrain=xgb.DMatrix(train_x[start:start+batch_size], train_y[start:start+batch_size]), xgb_model=model)\n",
    "\n",
    "        y_pr = model.predict(xgb.DMatrix(train_x))\n",
    "        #print('    MSE itr@{}: {}'.format(int(start/batch_size), sklearn.metrics.mean_squared_error(y_te, y_pr)))\n",
    "    print('MSE itr@{}: {}'.format(i, mean_squared_error(train_y, y_pr)))\n",
    "\n",
    "#y_pr = model.predict(xgb.DMatrix(x_te))\n",
    "#print('MSE at the end: {}'.format(mean_squared_error(test_y, y_pr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Attempt to find optimum depth, child weight (never finished processing)\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
    "\n",
    "\n",
    "param_test1 =   {\n",
    " 'max_depth':list(range(3,10,2)),\n",
    " 'min_child_weight':list(range(1,6,2))\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    " objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    " param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(train_x,train_y)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_prediction = model.predict(dtest)\n",
    "submission = pd.DataFrame(np.column_stack([test.id, final_prediction]), columns = ['id','log_price'])\n",
    "submission.to_csv(\"fourth_submission.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Download images\n",
    "\n",
    "import os \n",
    "import requests\n",
    "\n",
    "\n",
    "for i,row in test.iterrows():  \n",
    "    if isinstance(row['thumbnail_url'],str) and not os.path.exists('./images/{}.jpg'.format(row['id'])):\n",
    "        print(i)\n",
    "        url = row['thumbnail_url']\n",
    "        filename = './images/{}.jpg'.format(row['id'])\n",
    "        try:\n",
    "            r = requests.get(url, timeout=1.5)\n",
    "            if r.status_code == 200:\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "        except:\n",
    "            print('timeout')\n",
    "            next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Correlation matrix\n",
    "\n",
    "%matplotlib inline\n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "train\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = train.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##With random forest \n",
    "\n",
    "#from test script\n",
    "0.16969142613\n",
    "0.172199948847\n",
    "0.171665702104\n",
    "\n",
    "#with NLP \n",
    "0.170133091601\n",
    "0.172209588905\n",
    "0.171824024348\n",
    "\n",
    "\n",
    "#with amenites\n",
    "0.203165992454\n",
    "0.205297239503\n",
    "0.201059075764\n",
    "\n",
    "\n",
    "#with RGB\n",
    "0.169998647333\n",
    "0.171589382807\n",
    "0.171464521269\n",
    "\n",
    "\n",
    "#with census median income\n",
    "0.167652666241\n",
    "0.169194518538\n",
    "0.169065016674\n",
    "\n",
    "#with host response rate\n",
    "0.166819384931\n",
    "0.1681722164\n",
    "0.168567149508\n",
    "\n",
    "#with instant bookable\n",
    "0.166553242784\n",
    "0.168167160006\n",
    "0.168633984499\n",
    "\n",
    "0.166422024511\n",
    "0.168085883256\n",
    "0.168238200829\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
